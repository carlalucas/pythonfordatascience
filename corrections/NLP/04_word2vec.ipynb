{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M√©thodes de vectorisation : comptages et word embeddings\n",
        "\n",
        "Lino Galiana  \n",
        "2024-09-23"
      ],
      "id": "f9b81c57-fac0-4dde-93de-a74df0be7202"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p class=\"badges\">\n",
        "\n",
        "<a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/04_word2vec.ipynb\" class=\"github\"><i class=\"fab fa-github\"></i></a>\n",
        "<a href=\"https://downgit.github.io/#/home?url=https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/04_word2vec.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Download-Notebook-important?logo=Jupyter\" alt=\"Download\"></a>\n",
        "<a href=\"https://nbviewer.jupyter.org/github/linogaliana/python-datascientist-notebooksblob/main/notebooks/NLP/04_word2vec.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/Visualize-nbviewer-blue?logo=Jupyter\" alt=\"nbviewer\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-pytorch?autoLaunch=true&onyxia.friendlyName=%C2%AB04_word2vec%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-jupyter.sh%C2%BB&init.personalInitArgs=%C2%ABNLP%2004_word2vec%C2%BB&security.allowlist.enabled=false\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Tester_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-pytorch?autoLaunch=true&onyxia.friendlyName=%C2%AB04_word2vec%C2%BB&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmaster%2Fsspcloud%2Finit-vscode.sh%C2%BB&init.personalInitArgs=%C2%ABNLP%2004_word2vec%C2%BB&security.allowlist.enabled=false\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Tester_avec_VSCode-blue?logo=visualstudiocode&logoColor=blue\" alt=\"Onyxia\"></a><br>\n",
        "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks/blob/main/notebooks/NLP/04_word2vec.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "<a href=\"https://github.dev/linogaliana/python-datascientist-notebooks/notebooks/NLP/04_word2vec.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Open%20in%20Visual%20Studio%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc\" alt=\"githubdev\"></a>\n",
        "\n",
        "</p>\n",
        "\n",
        "</p>"
      ],
      "id": "c8347d03-465f-46b2-b3e7-45ab183ca0a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\" role=\"alert\">\n",
        "<h3 class=\"alert-heading\"><i class=\"fa-solid fa-triangle-exclamation\"></i> Warning</h3>\n",
        "\n",
        "Ce chapitre va √©voluer prochainement.\n",
        "\n",
        "</div>\n",
        "\n",
        "Cette page approfondit certains aspects pr√©sent√©s dans la\n",
        "[partie introductive](../../content/NLP/02_exoclean.qmd). Apr√®s avoir travaill√© sur le\n",
        "*Comte de Monte Cristo*, on va continuer notre exploration de la litt√©rature\n",
        "avec cette fois des auteurs anglophones :\n",
        "\n",
        "-   Edgar Allan Poe, (EAP) ;\n",
        "-   HP Lovecraft (HPL) ;\n",
        "-   Mary Wollstonecraft Shelley (MWS).\n",
        "\n",
        "Les donn√©es sont disponibles sur un CSV mis √† disposition sur [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv). L‚ÄôURL pour les r√©cup√©rer directement est\n",
        "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
        "\n",
        "Le but va √™tre dans un premier temps de regarder dans le d√©tail les termes les plus fr√©quents utilis√©s par les auteurs et de les repr√©senter graphiquement, puis on va ensuite essayer de pr√©dire quel texte correspond √† quel auteur √† partir de diff√©rents mod√®les de vectorisation, notamment les *word embeddings*.\n",
        "\n",
        "Ce chapitre s‚Äôinspire de plusieurs ressources disponibles en ligne:\n",
        "\n",
        "-   Un [premier *notebook* sur `Kaggle`](https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras)\n",
        "    et un [deuxi√®me](https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook) ;\n",
        "-   Un [d√©p√¥t `Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg) ;\n",
        "\n",
        "# 1. Packages √† installer\n",
        "\n",
        "Comme dans la [partie pr√©c√©dente](../../content/NLP/02_exoclean.qmd), il faut t√©l√©charger des librairies\n",
        "sp√©cialis√©ees pour le NLP, ainsi que certaines de leurs d√©pendances."
      ],
      "id": "c2ca0035-e5cb-4bfa-be0a-b7465759d969"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scipy==1.12 gensim sentence_transformers pandas matplotlib seaborn"
      ],
      "id": "ddb08e25"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensuite, comme nous allons utiliser la librairie `SpaCy` avec un corpus de textes\n",
        "en Anglais, il convient de t√©l√©charger le mod√®le NLP pour l‚ÄôAnglais. Pour cela,\n",
        "on peut se r√©f√©rer √† [la documentation de `SpaCy`](https://spacy.io/usage/models),\n",
        "extr√™mement bien faite."
      ],
      "id": "6a628ad5-907d-47cd-9e3e-19d716aae34c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "id": "8c1a7722"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "id": "0d641e88"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Nettoyage des donn√©es\n",
        "\n",
        "Nous allons ainsi √† nouveau utiliser le jeu de donn√©es `spooky` :"
      ],
      "id": "5d09c4a9-ff68-4fa8-b3ed-3c21cdbfd0e0"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_url = \"https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv\"\n",
        "spooky_df = pd.read_csv(data_url)\n"
      ],
      "id": "fa456b41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le jeu de donn√©es met ainsi en regard un auteur avec une phrase qu‚Äôil a √©crite :"
      ],
      "id": "887dd1d9-1476-49b7-8a97-f64a85a95831"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "spooky_df.head()\n"
      ],
      "id": "6fd9b65d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Preprocessing\n",
        "\n",
        "En NLP, la premi√®re √©tape est souvent celle du *preprocessing*, qui inclut notamment les √©tapes de tokenization et de nettoyage du texte. Comme celles-ci ont √©t√© vues en d√©tail dans le pr√©c√©dent chapitre, on se contentera ici d‚Äôun *preprocessing* minimaliste : suppression de la ponctuation et des *stop words* (pour la visualisation et les m√©thodes de vectorisation bas√©es sur des comptages).\n",
        "\n",
        "Jusqu‚Äô√† pr√©sent, nous avons utilis√© principalement `nltk` pour le\n",
        "*preprocessing* de donn√©es textuelles. Cette fois, nous proposons\n",
        "d‚Äôutiliser la librairie `spaCy` qui permet de mieux automatiser sous forme de\n",
        "*pipelines* de *preprocessing*.\n",
        "\n",
        "Pour initialiser le processus de nettoyage,\n",
        "on va utiliser le corpus `en_core_web_sm` (voir plus\n",
        "haut pour l‚Äôinstallation de ce corpus):"
      ],
      "id": "5374475a-84e4-40ed-8431-7c683c3b6fba"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "id": "f3617719"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On va utiliser un `pipe` `spacy` qui permet d‚Äôautomatiser, et de parall√©liser,\n",
        "un certain nombre d‚Äôop√©rations. Les *pipes* sont l‚Äô√©quivalent, en NLP, de\n",
        "nos *pipelines* `scikit` ou des *pipes* `pandas`. Il s‚Äôagit donc d‚Äôun outil\n",
        "tr√®s appropri√© pour industrialiser un certain nombre d‚Äôop√©rations de\n",
        "*preprocessing* :"
      ],
      "id": "065a3b2a-b36f-41f1-8cc3-f81ee18c4a18"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_docs(texts, remove_stopwords=False, n_process=4):\n",
        "\n",
        "    docs = nlp.pipe(\n",
        "        texts, n_process=n_process, disable=[\"parser\", \"ner\", \"lemmatizer\", \"textcat\"]\n",
        "    )\n",
        "    stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "    docs_cleaned = []\n",
        "    for doc in docs:\n",
        "        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n",
        "        if remove_stopwords:\n",
        "            tokens = [tok for tok in tokens if tok not in stopwords]\n",
        "        doc_clean = \" \".join(tokens)\n",
        "        docs_cleaned.append(doc_clean)\n",
        "\n",
        "    return docs_cleaned\n"
      ],
      "id": "8e728839"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On applique la fonction `clean_docs` √† notre colonne `pandas`.\n",
        "Les `pandas.Series` √©tant it√©rables, elles se comportent comme des listes et\n",
        "fonctionnent ainsi tr√®s bien avec notre `pipe` `spacy`."
      ],
      "id": "e42d89da-db02-409c-9697-69e22da926b3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "spooky_df[\"text_clean\"] = clean_docs(spooky_df[\"text\"])\n"
      ],
      "id": "0c4f4f27"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "spooky_df.head()\n"
      ],
      "id": "1626df7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Encodage de la variable √† pr√©dire\n",
        "\n",
        "On r√©alise un simple encodage de la variable √† pr√©dire :\n",
        "il y a trois cat√©gories (auteurs), repr√©sent√©es par des entiers 0, 1 et 2.\n",
        "\n",
        "Pour cela, on utilise le `LabelEncoder` de `scikit` d√©j√† pr√©sent√©\n",
        "dans la [partie mod√©lisation](#preprocessing). On va utiliser la m√©thode\n",
        "`fit_transform` qui permet, en un tour de main, d‚Äôappliquer √† la fois\n",
        "l‚Äôentra√Ænement (`fit`), √† savoir la cr√©ation d‚Äôune correspondance entre valeurs\n",
        "num√©riques et *labels*, et l‚Äôappliquer (`transform`) √† la m√™me colonne."
      ],
      "id": "8ee14e25-55bf-4a9e-9436-1433c7e41094"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "spooky_df[\"author_encoded\"] = le.fit_transform(spooky_df[\"author\"])\n"
      ],
      "id": "7edf138c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut v√©rifier les classes de notre `LabelEncoder` :"
      ],
      "id": "6af41f9b-3335-431f-9c9a-b58a58bee08d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "le.classes_\n"
      ],
      "id": "c55453b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Construction des bases d‚Äôentra√Ænement et de test\n",
        "\n",
        "On met de c√¥t√© un √©chantillon de test (20 %) avant toute analyse (m√™me descriptive).\n",
        "Cela permettra d‚Äô√©valuer nos diff√©rents mod√®les toute √† la fin de mani√®re tr√®s rigoureuse,\n",
        "puisque ces donn√©es n‚Äôauront jamais utilis√©es pendant l‚Äôentra√Ænement."
      ],
      "id": "f52d4a58-e622-4655-af51-396772a42eaf"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = spooky_df[\"author_encoded\"]\n",
        "X = spooky_df[\"text_clean\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "id": "62713649"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notre √©chantillon initial n‚Äôest pas √©quilibr√© (*balanced*) : on retrouve plus d‚Äôoeuvres de\n",
        "certains auteurs que d‚Äôautres. Afin d‚Äôobtenir un mod√®le qui soit √©valu√© au mieux, nous allons donc stratifier notre √©chantillon de mani√®re √† obtenir une r√©partition similaire d‚Äôauteurs dans nos\n",
        "ensembles d‚Äôentra√Ænement et de test.\n",
        "\n",
        "Aper√ßu du premier √©l√©ment de `X_train` :"
      ],
      "id": "a9b4652e-7b54-44bb-9604-f63e87694b45"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train[0]\n"
      ],
      "id": "27862c7e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut aussi v√©rifier qu‚Äôon est capable de retrouver\n",
        "la correspondance entre nos auteurs initiaux avec\n",
        "la m√©thode `inverse_transform` :"
      ],
      "id": "dd318ca5-0c27-4eb5-bdd9-e2df6596ce1f"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_train[0], le.inverse_transform([y_train[0]])[0])\n"
      ],
      "id": "83c24f8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Statistiques exploratoires\n",
        "\n",
        "## 3.1 R√©partition des labels\n",
        "\n",
        "Refaisons un graphique que nous avons d√©j√† produit pr√©c√©demment pour voir\n",
        "la r√©partition de notre corpus entre auteurs :"
      ],
      "id": "ea8d6afc-96f6-41c5-a67f-eca1b67ff993"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = pd.Series(le.inverse_transform(y_train)).value_counts().plot(kind=\"bar\")\n",
        "fig\n"
      ],
      "id": "9252b12b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On observe une petite asym√©trie : les passages des livres d‚ÄôEdgar Allen Poe sont plus nombreux que ceux des autres auteurs dans notre corpus d‚Äôentra√Ænement, ce qui peut √™tre probl√©matique dans le cadre d‚Äôune t√¢che de classification.\n",
        "L‚Äô√©cart n‚Äôest pas dramatique, mais on essaiera d‚Äôen tenir compte dans l‚Äôanalyse en choisissant une m√©trique d‚Äô√©valuation pertinente.\n",
        "\n",
        "## 3.2 Mots les plus fr√©quemment utilis√©s par chaque auteur\n",
        "\n",
        "On va supprimer les *stop words* pour r√©duire le bruit dans notre jeu\n",
        "de donn√©es."
      ],
      "id": "d15a1e0d-4e2c-44f7-bf98-6df8044f34f6"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppression des stop words\n",
        "X_train_no_sw = clean_docs(X_train, remove_stopwords=True)\n",
        "X_train_no_sw = np.array(X_train_no_sw)\n"
      ],
      "id": "874a2a9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour visualiser rapidement nos corpus, on peut utiliser la technique des\n",
        "nuages de mots d√©j√† vue √† plusieurs reprises.\n",
        "\n",
        "Vous pouvez essayer de faire vous-m√™me les nuages ci-dessous\n",
        "ou cliquer sur la ligne ci-dessous pour afficher le code ayant\n",
        "g√©n√©r√© les figures :"
      ],
      "id": "c84eb5da-6d44-425d-87ed-3ee85d87e19f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details><summary><code>Cliquer pour afficher le code</code> üëá</summary>\n",
        "\n",
        "``` python\n",
        "def plot_top_words(initials, ax, n_words=20):\n",
        "    # Calcul des mots les plus fr√©quemment utilis√©s par l'auteur\n",
        "    texts = X_train_no_sw[le.inverse_transform(y_train) == initials]\n",
        "    all_tokens = \" \".join(texts).split()\n",
        "    counts = Counter(all_tokens)\n",
        "    top_words = [word[0] for word in counts.most_common(n_words)]\n",
        "    top_words_counts = [word[1] for word in counts.most_common(n_words)]\n",
        "\n",
        "    # Repr√©sentation sous forme de barplot\n",
        "    ax = sns.barplot(ax=ax, x=top_words, y=top_words_counts)\n",
        "    ax.set_title(f\"Most Common Words used by {initials_to_author[initials]}\")\n",
        "```\n",
        "\n",
        "``` python\n",
        "initials_to_author = {\n",
        "    \"EAP\": \"Edgar Allen Poe\",\n",
        "    \"HPL\": \"H.P. Lovecraft\",\n",
        "    \"MWS\": \"Mary Wollstonecraft Shelley\",\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, figsize=(12, 12))\n",
        "\n",
        "plot_top_words(\"EAP\", ax=axs[0])\n",
        "plot_top_words(\"HPL\", ax=axs[1])\n",
        "plot_top_words(\"MWS\", ax=axs[2])\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "id": "1048898b-90f1-4665-8b74-0a3bdb55e156"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beaucoup de mots se retrouvent tr√®s utilis√©s par les trois auteurs.\n",
        "Il y a cependant des diff√©rences notables : le mot *‚Äúlife‚Äù*\n",
        "est le plus employ√© par MWS, alors qu‚Äôil n‚Äôappara√Æt pas dans les deux autres tops.\n",
        "De m√™me, le mot *‚Äúold‚Äù* est le plus utilis√© par HPL\n",
        "l√† o√π les deux autres ne l‚Äôutilisent pas de mani√®re surrepr√©sent√©e.\n",
        "\n",
        "Il semble donc qu‚Äôil y ait des particularit√©s propres √† chacun des auteurs\n",
        "en termes de vocabulaire,\n",
        "ce qui laisse penser qu‚Äôil est envisageable de pr√©dire les auteurs √† partir\n",
        "de leurs textes dans une certaine mesure.\n",
        "\n",
        "# 4. Pr√©diction sur le set d‚Äôentra√Ænement\n",
        "\n",
        "Nous allons √† pr√©sent v√©rifier cette conjecture en comparant\n",
        "plusieurs mod√®les de vectorisation,\n",
        "*i.e.* de transformation du texte en objets num√©riques\n",
        "pour que l‚Äôinformation contenue soit exploitable dans un mod√®le de classification.\n",
        "\n",
        "## 4.1 D√©marche\n",
        "\n",
        "Comme nous nous int√©ressons plus √† l‚Äôeffet de la vectorisation qu‚Äô√† la t√¢che de classification en elle-m√™me,\n",
        "nous allons utiliser un algorithme de classification simple (un SVM lin√©aire), avec des param√®tres non fine-tun√©s (c‚Äôest-√†-dire des param√®tres pas n√©cessairement choisis pour √™tre les meilleurs de tous)."
      ],
      "id": "befcbb4a-a39e-477e-ab4d-bbb221f678fe"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = LinearSVC(max_iter=10000, C=0.1, dual=\"auto\")\n"
      ],
      "id": "bc5bb3fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ce mod√®le est connu pour √™tre tr√®s performant sur les t√¢ches de classification de texte, et nous fournira donc un bon mod√®le de r√©f√©rence (*baseline*). Cela nous permettra √©galement de comparer de mani√®re objective l‚Äôimpact des m√©thodes de vectorisation sur la performance finale.\n",
        "\n",
        "Pour les deux premi√®res m√©thodes de vectorisation\n",
        "(bas√©es sur des fr√©quences et fr√©quences relatives des mots),\n",
        "on va simplement normaliser les donn√©es d‚Äôentr√©e, ce qui va permettre au SVM de converger plus rapidement, ces mod√®les √©tant sensibles aux diff√©rences d‚Äô√©chelle dans les donn√©es.\n",
        "\n",
        "On va √©galement *fine-tuner* via *grid-search*\n",
        "certains hyperparam√®tres li√©s √† ces m√©thodes de vectorisation :\n",
        "\n",
        "-   on teste diff√©rents *ranges* de `n-grams` (unigrammes et unigrammes + bigrammes)\n",
        "-   on teste avec et sans *stop-words*\n",
        "\n",
        "Afin d‚Äô√©viter le surapprentissage,\n",
        "on va √©valuer les diff√©rents mod√®les via validation crois√©e, calcul√©e sur 4 blocs.\n",
        "\n",
        "On r√©cup√®re √† la fin le meilleur mod√®le selon une m√©trique sp√©cifi√©e.\n",
        "On choisit le `score F1`,\n",
        "moyenne harmonique de la pr√©cision et du rappel,\n",
        "qui donne un poids √©quilibr√© aux deux m√©triques, tout en p√©nalisant fortement le cas o√π l‚Äôune des deux est faible.\n",
        "Pr√©cis√©ment, on retient le `score F1 *micro-averaged*` :\n",
        "les contributions des diff√©rentes classes √† pr√©dire sont agr√©g√©es,\n",
        "puis on calcule le `score F1` sur ces donn√©es agr√©g√©es.\n",
        "L‚Äôavantage de ce choix est qu‚Äôil permet de tenir compte des diff√©rences\n",
        "de fr√©quences des diff√©rentes classes.\n",
        "\n",
        "## 4.2 Pipeline de pr√©diction\n",
        "\n",
        "On va utiliser un *pipeline* `scikit` ce qui va nous permettre d‚Äôavoir\n",
        "un code tr√®s concis pour effectuer cet ensemble de t√¢ches coh√©rentes.\n",
        "De plus, cela va nous assurer de g√©rer de mani√®re coh√©rente nos diff√©rentes\n",
        "transformations (cf.¬†[partie sur les pipelines](#pipelines))\n",
        "\n",
        "Pour se faciliter la vie, on d√©finit une fonction `fit_vectorizers` qui\n",
        "int√®gre dans un *pipeline* g√©n√©rique une m√©thode d‚Äôestimation `scikit`\n",
        "et fait de la validation crois√©e en cherchant le meilleur mod√®le\n",
        "(en excluant/incluant les *stop words* et avec unigrammes/bigrammes)"
      ],
      "id": "20d30aad-ee33-4761-872a-6cb61f6d2f77"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_vectorizers(vectorizer):\n",
        "    pipeline = Pipeline(\n",
        "        [\n",
        "            (\"vect\", vectorizer()),\n",
        "            (\"scaling\", StandardScaler(with_mean=False)),\n",
        "            (\"clf\", clf),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    parameters = {\n",
        "        \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
        "        \"vect__stop_words\": (\"english\", None),\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline, parameters, scoring=\"f1_micro\", cv=4, n_jobs=4, verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_parameters = grid_search.best_estimator_.get_params()\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "\n",
        "    print(f\"CV scores {grid_search.cv_results_['mean_test_score']}\")\n",
        "    print(f\"Mean F1 {np.mean(grid_search.cv_results_['mean_test_score'])}\")\n",
        "\n",
        "    return grid_search\n"
      ],
      "id": "4b1c3553"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Approche *bag-of-words*\n",
        "\n",
        "On commence par une approche **‚Äúbag-of-words‚Äù**,\n",
        "i.e.¬†qui revient simplement √† repr√©senter chaque document par un vecteur\n",
        "qui compte le nombre d‚Äôapparitions de chaque mot du vocabulaire dans le document.\n",
        "\n",
        "Illustrons d‚Äôabord le principe √† l‚Äôaide d‚Äôun exemple simple."
      ],
      "id": "3fb27630-8e8b-4dc2-b8b7-0f30e63f5765"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Un premier document √† propos des chats.\",\n",
        "    \"Un second document qui parle des chiens.\",\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(corpus)\n"
      ],
      "id": "f3f5ee39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L‚Äôobjet `vectorizer` a √©t√© ‚Äúentra√Æn√©‚Äù (*fit*) sur notre corpus d‚Äôexemple contenant deux documents. Il a notamment appris le vocabulaire complet du corpus, dont on peut afficher l‚Äôordre."
      ],
      "id": "377b2bd3-ba02-42da-ad9d-e7266b185026"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()\n"
      ],
      "id": "c74aaffb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L‚Äôobjet `vectorizer` entra√Æn√© peut maintenant vectoriser le corpus initial, selon l‚Äôordre du vocabulaire affich√© ci-dessus."
      ],
      "id": "946f69c3-971c-41ee-bb88-3a53ba0d6e28"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = vectorizer.transform(corpus)\n",
        "print(X.toarray())\n"
      ],
      "id": "77d8bcdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quel score `F1` obtient-on finalement avec cette m√©thode de vectorisation sur notre probl√®me de classification d‚Äôauteurs ?"
      ],
      "id": "87e0bb1e-9732-41d3-ae76-ee8dd98f370e"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_bow = fit_vectorizers(CountVectorizer)\n"
      ],
      "id": "511ad313"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. TF-IDF\n",
        "\n",
        "On s‚Äôint√©resse ensuite √† l‚Äôapproche **TF-IDF**,\n",
        "qui permet de tenir compte des fr√©quences *relatives* des mots.\n",
        "\n",
        "Ainsi, pour un mot donn√©, on va multiplier la fr√©quence d‚Äôapparition du mot dans le document (calcul√© comme dans la m√©thode pr√©c√©dente) par un terme qui p√©nalise une fr√©quence √©lev√©e du mot dans le corpus. L‚Äôimage ci-dessous, emprunt√©e √† Chris Albon, illustre cette mesure:\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/tfidf.png)\n",
        "\n",
        "*Source: [Towards Data Science](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)*\n",
        "\n",
        "La vectorisation `TF-IDF` permet donc de limiter l‚Äôinfluence des *stop-words*\n",
        "et donc de donner plus de poids aux mots les plus salients d‚Äôun document.\n",
        "Illustrons cela √† nouveau avec notre corpus d‚Äôexemple de deux documents."
      ],
      "id": "c383baee-5efe-4e76-a029-623a74e036da"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Un premier document √† propos des chats.\",\n",
        "    \"Un second document qui parle des chiens.\",\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n"
      ],
      "id": "93dceb9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L√† encore, le vectoriseur a ‚Äúappris‚Äù le vocabulaire du corpus."
      ],
      "id": "81a675ca-9d4e-49c3-bb62-86f81bdbbf1c"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()\n"
      ],
      "id": "488726eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et peut √™tre utilis√© pour calculer les scores TF-IDF de chacun des termes des documents."
      ],
      "id": "a4fba5fe-0c60-44aa-920e-c1b966c47863"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = vectorizer.transform(corpus)\n",
        "print(X.toarray())\n"
      ],
      "id": "7e2b3c4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque que ‚Äúchats‚Äù et ‚Äúchiens‚Äù poss√®dent les scores les plus √©lev√©s, ce sont bien les termes les plus distinctifs. A l‚Äôinverse, les termes qui reviennent dans les deux documents (‚Äúun‚Äù, ‚Äúdocument‚Äù, ‚Äúdes‚Äù) ont un score inf√©rieur, car ils sont beaucoup pr√©sents dans le corpus relativement.\n",
        "\n",
        "Quel score `F1` obtient-on avec cette m√©thode de vectorisation sur notre probl√®me de classification d‚Äôauteurs ?"
      ],
      "id": "a5b41578-ea7e-4686-8710-dfdaea10392f"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_tfidf = fit_vectorizers(TfidfVectorizer)\n"
      ],
      "id": "b3d99c4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On observe clairement que la performance de classification est bien sup√©rieure,\n",
        "ce qui montre la pertinence de cette technique.\n",
        "\n",
        "# 7. Word2vec avec averaging\n",
        "\n",
        "On va maintenant explorer les techniques de vectorisation bas√©es sur les\n",
        "*embeddings* de mots, et notamment la plus populaire : `Word2Vec`.\n",
        "\n",
        "L‚Äôid√©e derri√®re est simple, mais a r√©volutionn√© le NLP :\n",
        "au lieu de repr√©senter les documents par des\n",
        "vecteurs *sparse* de tr√®s grande dimension (la taille du vocabulaire)\n",
        "comme on l‚Äôa fait jusqu‚Äô√† pr√©sent,\n",
        "on va les repr√©senter par des vecteurs *dense* (continus)\n",
        "de dimension r√©duite (en g√©n√©ral, autour de 100-300).\n",
        "\n",
        "Chacune de ces dimensions va repr√©senter un facteur latent,\n",
        "c‚Äôest √† dire une variable inobserv√©e,\n",
        "de la m√™me mani√®re que les composantes principales produites par une ACP.\n",
        "\n",
        "![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/w2v_vecto.png)\n",
        "\n",
        "*Source: [Medium](https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d)*\n",
        "\n",
        "**Pourquoi est-ce int√©ressant ?**\n",
        "Pour de nombreuses raisons, mais pour r√©sumer :\n",
        "cela permet de beaucoup mieux capturer la similarit√© s√©mantique entre les documents.\n",
        "\n",
        "Par exemple, un humain sait qu‚Äôun document contenant le mot *‚ÄúRoi‚Äù*\n",
        "et un autre document contenant le mot *‚ÄúReine‚Äù* ont beaucoup de chance\n",
        "d‚Äôaborder des sujets semblables.\n",
        "\n",
        "Pourtant, une vectorisation de type comptage ou TF-IDF\n",
        "ne permet pas de saisir cette similarit√© :\n",
        "le calcul d‚Äôune mesure de similarit√© (norme euclidienne ou similarit√© cosinus)\n",
        "entre les deux vecteurs ne prendra en compte la similarit√© des deux concepts, puisque les mots utilis√©s sont diff√©rents.\n",
        "\n",
        "A l‚Äôinverse, un mod√®le `word2vec` bien entra√Æn√© va capter\n",
        "qu‚Äôil existe un facteur latent de type *‚Äúroyaut√©‚Äù*,\n",
        "et la similarit√© entre les vecteurs associ√©s aux deux mots sera forte.\n",
        "\n",
        "La magie va m√™me plus loin : le mod√®le captera aussi qu‚Äôil existe un\n",
        "facteur latent de type *‚Äúgenre‚Äù*,\n",
        "et va permettre de construire un espace s√©mantique dans lequel les\n",
        "relations arithm√©tiques entre vecteurs ont du sens ;\n",
        "par exemple :\n",
        "$$\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}$$\n",
        "\n",
        "**Comment ces mod√®les sont-ils entra√Æn√©s ?**\n",
        "Via une t√¢che de pr√©diction r√©solue par un r√©seau de neurones simple.\n",
        "\n",
        "L‚Äôid√©e fondamentale est que la signification d‚Äôun mot se comprend\n",
        "en regardant les mots qui apparaissent fr√©quemment dans son voisinage.\n",
        "\n",
        "Pour un mot donn√©, on va donc essayer de pr√©dire les mots\n",
        "qui apparaissent dans une fen√™tre autour du mot cible.\n",
        "\n",
        "En r√©p√©tant cette t√¢che de nombreuses fois et sur un corpus suffisamment vari√©,\n",
        "on obtient finalement des *embeddings* pour chaque mot du vocabulaire,\n",
        "qui pr√©sentent les propri√©t√©s discut√©es pr√©c√©demment."
      ],
      "id": "1e34d926-a01d-462a-933d-697b053133a1"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tokens = [text.split() for text in X_train]\n",
        "w2v_model = Word2Vec(X_train_tokens, vector_size=200, window=5, min_count=1, workers=4)\n"
      ],
      "id": "8d5e52b5"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(\"mother\")\n"
      ],
      "id": "7da374f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On voit que les mots les plus similaires √† *‚Äúmother‚Äù*\n",
        "sont souvent des mots li√©s √† la famille, mais pas toujours.\n",
        "\n",
        "C‚Äôest li√© √† la taille tr√®s restreinte du corpus sur lequel on entra√Æne le mod√®le,\n",
        "qui ne permet pas de r√©aliser des associations toujours pertinentes.\n",
        "\n",
        "L‚Äô*embedding* (la repr√©sentation vectorielle) de chaque document correspond √† la moyenne des *word-embeddings* des mots qui le composent :"
      ],
      "id": "6a544a51-c0ac-491a-9e00-4ac3fcfe6af8"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mean_vector(w2v_vectors, words):\n",
        "    words = [word for word in words if word in w2v_vectors]\n",
        "    if words:\n",
        "        avg_vector = np.mean(w2v_vectors[words], axis=0)\n",
        "    else:\n",
        "        avg_vector = np.zeros_like(w2v_vectors[\"hi\"])\n",
        "    return avg_vector\n",
        "\n",
        "\n",
        "def fit_w2v_avg(w2v_vectors):\n",
        "    X_train_vectors = np.array(\n",
        "        [get_mean_vector(w2v_vectors, words) for words in X_train_tokens]\n",
        "    )\n",
        "\n",
        "    scores = cross_val_score(\n",
        "        clf, X_train_vectors, y_train, cv=4, scoring=\"f1_micro\", n_jobs=4\n",
        "    )\n",
        "\n",
        "    print(f\"CV scores {scores}\")\n",
        "    print(f\"Mean F1 {np.mean(scores)}\")\n",
        "    return scores\n"
      ],
      "id": "0bce1a0f"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_w2vec = fit_w2v_avg(w2v_model.wv)\n"
      ],
      "id": "8c11bf4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La performance chute fortement ;\n",
        "la faute √† la taille tr√®s restreinte du corpus, comme annonc√© pr√©c√©demment.\n",
        "\n",
        "# 8. Word2vec pr√©-entra√Æn√© + averaging\n",
        "\n",
        "Quand on travaille avec des corpus de taille restreinte,\n",
        "c‚Äôest g√©n√©ralement une mauvaise id√©e d‚Äôentra√Æner son propre mod√®le `word2vec`.\n",
        "\n",
        "Heureusement, des mod√®les pr√©-entra√Æn√©s sur de tr√®s gros corpus sont disponibles.\n",
        "Ils permettent de r√©aliser du *transfer learning*,\n",
        "c‚Äôest-√†-dire de b√©n√©ficier de la performance d‚Äôun mod√®le qui a √©t√© entra√Æn√© sur une autre t√¢che ou bien sur un autre corpus.\n",
        "\n",
        "L‚Äôun des mod√®les les plus connus pour d√©marrer est le `glove_model` de\n",
        "`Gensim` (Glove pour *Global Vectors for Word Representation*)[1]:\n",
        "\n",
        "> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
        ">\n",
        "> *Source* : https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "On peut le charger directement gr√¢ce √† l‚Äôinstruction suivante :\n",
        "\n",
        "[1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. *GloVe: Global Vectors for Word Representation*."
      ],
      "id": "55e73581-a506-4d43-99b5-24a84483a07b"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_model = gensim.downloader.load(\"glove-wiki-gigaword-200\")\n"
      ],
      "id": "6d040f0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Par exemple, la repr√©sentation vectorielle de roi est l‚Äôobjet\n",
        "multidimensionnel suivant :"
      ],
      "id": "3975d16f-1bbd-4cb9-8ccf-1f10d6007367"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_model[\"king\"]\n"
      ],
      "id": "2018360f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme elle est peu intelligible, on va plut√¥t rechercher les termes les\n",
        "plus similaires. Par exemple,"
      ],
      "id": "db3b2c8b-ea38-4909-9664-e20181c7ffc2"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_model.most_similar(\"mother\")\n"
      ],
      "id": "294dd1ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On peut retrouver notre formule pr√©c√©dente\n",
        "\n",
        "$$\\text{king} - \\text{man} + \\text{woman} ‚âà \\text{queen}$$\n",
        "dans ce plongement de mots:"
      ],
      "id": "f02c5275-9564-4b35-aedd-a95591f087e3"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n"
      ],
      "id": "df193fcf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vous pouvez vous r√©f√©rer √† [ce tutoriel](https://jalammar.github.io/illustrated-word2vec/)\n",
        "pour en d√©couvrir plus sur `Word2Vec`.\n",
        "\n",
        "Faisons notre apprentissage par transfert :"
      ],
      "id": "9c0e1815-1a6b-40cd-b938-788f1a986e4b"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_w2vec_transfert = fit_w2v_avg(glove_model)\n"
      ],
      "id": "01e8ff8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La performance remonte substantiellement.\n",
        "Cela √©tant, on ne parvient pas √† faire mieux que les approches basiques,\n",
        "on arrive √† peine aux performances de la vectorisation par comptage.\n",
        "\n",
        "En effet, pour rappel, les performances sont les suivantes :"
      ],
      "id": "1f1b37f5-fb1d-42a1-8fe6-35f529914bdf"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "perfs = pd.DataFrame(\n",
        "    [\n",
        "        np.mean(cv_bow.cv_results_[\"mean_test_score\"]),\n",
        "        np.mean(cv_tfidf.cv_results_[\"mean_test_score\"]),\n",
        "        np.mean(cv_w2vec),\n",
        "        np.mean(cv_w2vec_transfert),\n",
        "    ],\n",
        "    index=[\n",
        "        \"Bag-of-Words\",\n",
        "        \"TF-IDF\",\n",
        "        \"Word2Vec non pr√©-entra√Æn√©\",\n",
        "        \"Word2Vec pr√©-entra√Æn√©\",\n",
        "    ],\n",
        "    columns=[\"Mean F1 score\"],\n",
        ").sort_values(\"Mean F1 score\", ascending=False)\n",
        "perfs\n"
      ],
      "id": "ba1a6f30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les performences limit√©es du mod√®le *Word2Vec* sont cette fois certainement dues √† la mani√®re dont\n",
        "les *word-embeddings* sont exploit√©s : ils sont moyenn√©s pour d√©crire chaque document.\n",
        "\n",
        "Cela a plusieurs limites :\n",
        "\n",
        "-   on ne tient pas compte de l‚Äôordre et donc du contexte des mots\n",
        "-   lorsque les documents sont longs, la moyennisation peut cr√©er\n",
        "    des repr√©sentation bruit√©es.\n",
        "\n",
        "# 9. Contextual embeddings\n",
        "\n",
        "Les *embeddings* contextuels visent √† pallier les limites des *embeddings*\n",
        "traditionnels √©voqu√©es pr√©c√©demment.\n",
        "\n",
        "Cette fois, les mots n‚Äôont plus de repr√©sentation vectorielle fixe,\n",
        "celle-ci est calcul√©e dynamiquement en fonction des mots du voisinage, et ainsi de suite.\n",
        "Cela permet de tenir compte de la structure des phrases\n",
        "et de tenir compte du fait que le sens d‚Äôun mot est fortement d√©pendant des mots\n",
        "qui l‚Äôentourent.\n",
        "Par exemple, dans les expressions ‚Äúle pr√©sident Macron‚Äù et ‚Äúle camembert Pr√©sident‚Äù le mot pr√©sident n‚Äôa pas du tout le m√™me r√¥le.\n",
        "\n",
        "Ces *embeddings* sont produits par des architectures tr√®s complexes,\n",
        "de type Transformer (`BERT`, etc.).\n",
        "\n",
        "*TODO: approfondir le sujet*"
      ],
      "id": "dfba7e5a-0c1b-4b0e-8aec-9f3521a58bf1"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-mpnet-base-v2\")\n"
      ],
      "id": "e4d0ca89"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verdict : on fait tr√®s l√©g√®rement mieux que la vectorisation TF-IDF.\n",
        "On voit donc l‚Äôimportance de tenir compte du contexte.\n",
        "\n",
        "**Mais pourquoi, avec une m√©thode tr√®s compliqu√©e, ne parvenons-nous pas √† battre une m√©thode toute simple ?**\n",
        "\n",
        "On peut avancer plusieurs raisons :\n",
        "\n",
        "-   le `TF-IDF` est un mod√®le simple, mais toujours tr√®s performant\n",
        "    (on parle de *‚Äútough-to-beat baseline‚Äù*).\n",
        "-   la classification d‚Äôauteurs est une t√¢che tr√®s particuli√®re et tr√®s ardue,\n",
        "    qui ne fait pas justice aux *embeddings*. Comme on l‚Äôa dit pr√©c√©demment, ces derniers se r√©v√®lent particuli√®rement pertinents lorsqu‚Äôil est question de similarit√© s√©mantique entre des textes (*clustering*, etc.).\n",
        "\n",
        "Dans le cas de notre t√¢che de classification, il est probable que\n",
        "certains mots (noms de personnage, noms de lieux) soient suffisants pour classifier de mani√®re pertinente,\n",
        "ce que ne permettent pas de capter les *embeddings* qui accordent √† tous les mots la m√™me importance.\n",
        "\n",
        "# 10. Aller plus loin\n",
        "\n",
        "-   Nous avons entra√Æn√© diff√©rents mod√®les sur l‚Äô√©chantillon d‚Äôentra√Ænement par validation crois√©e, mais nous n‚Äôavons toujours pas utilis√© l‚Äô√©chantillon test que nous avons mis de c√¥t√© au d√©but. R√©aliser la pr√©diction sur les donn√©es de test, et v√©rifier si l‚Äôon obtient le m√™me classement des m√©thodes de vectorisation.\n",
        "-   Faire un *vrai* split train/test : faire l‚Äôentra√Ænement avec des textes de certains auteurs, et faire la pr√©diction avec des textes d‚Äôauteurs diff√©rents. Cela permettrait de neutraliser la pr√©sence de noms de lieux, de personnages, etc.\n",
        "-   Comparer avec d‚Äôautres algorithmes de classification qu‚Äôun SVM\n",
        "-   (Avanc√©) : fine-tuner le mod√®le d‚Äôembeddings contextuels sur la t√¢che de classification"
      ],
      "id": "7989bac3-a166-4f00-a817-0fa5dd73161a"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  }
}